# ðŸ§  CLAM Inference Pipeline for WSIs â€“ Dockerized

This Docker container executes a full inference pipeline using a trained CLAM model for classification of Whole Slide Images (WSIs), following the methodology described by Kalimuthu et al. It performs:

1. Patch extraction from WSIs
2. Feature extraction from patches using a Vision Transformer
3. Slide-level inference using a trained CLAM model

---

## ðŸš€ Running the Docker Container

To run the container, you must:

1. Create two folders on your local machine:
    - **Input folder**: Local directory with the WSIs (in `.svs`, `.tif`, `.ndpi`, or `.tiff` format)
    - **Output folder**: Directory where all intermediate and final results will be saved

2. Provide the `--patch_level` parameter at runtime:
    - **Patch level** (`--patch_level`): The OpenSlide pyramid level used to extract patches (default is `1`, usually corresponds to 20x)

### ðŸ§¾ Required Parameters

| Parameter     | Description                                                                            |
|---------------|----------------------------------------------------------------------------------------|
| `-v /path/to/input:/app/input`  | Maps your local WSI folder to the containerâ€™s `/app/input` folder    |
| `-v /path/to/output:/app/output`| Maps your local results folder to `/app/output` inside the container |
| `--patch_level` | Level of downsampling (OpenSlide level)                             |

### âœ… `--patch_level` Parameter

This parameter defines the resolution level of the WSI to use when extracting patches. It corresponds to the pyramid levels generated by OpenSlide.

- `0`: Maximum resolution (typically 40x).
- `1`: 2x downsampled (typically 20x) â†’ **default and recommended**.

> Use `--patch_level 1` unless you're certain your images require a different scale.

### Example Inference Command

```bash
docker run --rm \
  -v /path/to/local/input:/app/input \
  -v /path/to/local/output:/app/output \
  kalimuthu_infer_image \
  --patch_level 1
```

> ðŸ–¼ï¸ kalimuthu_infer_image is the name of the Docker image.

> ðŸ” The --patch_level parameter is optional. If not specified, it will default to 1.

---

## ðŸ“ Output Folder Structure

After running, the output directory will contain:

```
output/
â”œâ”€â”€ patches/
â”‚   â”œâ”€â”€ patches/       # Extracted image patches
â”‚   â”œâ”€â”€ masks/         # Tissue masks
â”‚   â””â”€â”€ stitches/      # Optional tissue visualizations
â”œâ”€â”€ features/          # Extracted feature files (.pt)
    â”œâ”€â”€ h5_files/      # Contains the HDF5 files with patch metadata (location, size, etc.)
    â””â”€â”€ pt_files/      # Extracted feature tensors (.pt) used for model inference
â”œâ”€â”€ process_slides.csv # CSV with processed slide IDs
â””â”€â”€ inference_predictions.csv # Final slide-level predictions
```

### ðŸ“„ Description of Output Files

- **`inference_predictions.csv`**  
  Contains the final classification results for each slide. It includes the following columns:

  | Column                     | Description                                      |
  |----------------------------|--------------------------------------------------|
  | `slide_id`                 | ID of the processed slide                        |
  | `predicted_class`          | Predicted class label (0 or 1)                   |
  | `positive_class_probability` | Probability of the slide belonging to class 1 |
  
- **`process_slides.csv`**  
  Keeps track of which slides have already been processed, to avoid repeating patch extraction and feature computation.

  - âœ… If you add a new image, it will be appended to this file and processed as expected.
  - âš ï¸ If you want to **re-run the full pipeline** (patch extraction + feature extraction + inference) on a slide thatâ€™s already in the file, you **must manually delete that row** from `process_slides.csv`.

---

## âš™ï¸ GPU vs CPU Execution

This container runs with **GPU if available** and **falls back to CPU automatically** if not.

### ðŸ§  Note on Performance

- **GPU recommended** for fast feature extraction with Vision Transformer
- **CPU works**, but feature extraction will be significantly slower

### ðŸ› ï¸ Requirements for GPU Execution

To run with GPU, your local machine must have:

- A compatible **NVIDIA GPU**
- **NVIDIA drivers** installed
- **Docker Engine** installed
- **NVIDIA Container Toolkit** installed

#### Install NVIDIA Container Toolkit

```bash
sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker
```

#### Verify GPU Access from Docker

```bash
docker run --rm --gpus all nvidia/cuda:11.8.0-base nvidia-smi
```

If this command returns information about your GPU (model, memory, driver version, etc.), the system is correctly set up to use the GPU

> The inference script will automatically detect and use the GPU if available

---

## ðŸ§ª Minimal Example

### 1. Create local folders:

```bash
mkdir input output
```

### 2. Copy your WSI slides (e.g. `.svs`) into `input/`.

```bash
cp your_slide.svs input/
```

### 3. Run the container:

```bash
docker run --rm \
  -v /path/to/local/input:/app/input \
  -v /path/to/local/output:/app/output \
  kalimuthu_infer_image \
  --patch_level 1
```

### 4. View results in `output/inference_predictions.csv`:

```bash
cat output/inference_predictions.csv
```

---

## ðŸ“¬ Questions?

If you encounter any problems, please check:

- That your input slides are supported (`.svs`, `.tif`, etc.)
- That Docker has the right permissions to access input/output folders
- That your system has enough RAM/VRAM for patching and feature extraction

---

Â© 2025 â€“ CLAM WSI Dockerized Inference

