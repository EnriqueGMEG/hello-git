# 🧠 CLAM Inference Pipeline for WSIs – Dockerized

This Docker container executes a full inference pipeline using a trained CLAM model for classification of Whole Slide Images (WSIs), following the methodology described by Kalimuthu et al. It performs:

1. Patch extraction from WSIs
2. Feature extraction from patches using a Vision Transformer
3. Slide-level inference using a trained CLAM model

---

## 🚀 Running the Docker Container

To run the container, you must:

1. Create two folders on your local machine:
    - **Input folder**: Local directory with the WSIs (in `.svs`, `.tif`, `.ndpi`, or `.tiff` format)
    - **Output folder**: Directory where all intermediate and final results will be saved

2. Provide the `--patch_level` parameter at runtime:
    - **Patch level** (`--patch_level`): The OpenSlide pyramid level used to extract patches (default is `1`, usually corresponds to 20x)

### 🧾 Required Parameters

| Parameter     | Description                                                                            |
|---------------|----------------------------------------------------------------------------------------|
| `-v /path/to/input:/app/input`  | Maps your local WSI folder to the container’s `/app/input` folder    |
| `-v /path/to/output:/app/output`| Maps your local results folder to `/app/output` inside the container |
| `--patch_level` | Level of downsampling (OpenSlide level)                             |

### ✅ `--patch_level` Parameter

This parameter defines the resolution level of the WSI to use when extracting patches. It corresponds to the pyramid levels generated by OpenSlide.

- `0`: Maximum resolution (typically 40x).
- `1`: 2x downsampled (typically 20x) → **default and recommended**.

> Use `--patch_level 1` unless you're certain your images require a different scale.

### Example Inference Command

```bash
docker run --rm \
  -v /path/to/local/input:/app/input \
  -v /path/to/local/output:/app/output \
  kalimuthu_infer_image \
  --patch_level 1
```

> 🖼️ kalimuthu_infer_image is the name of the Docker image.

> 🔁 The --patch_level parameter is optional. If not specified, it will default to 1.

---

## 📁 Output Folder Structure

After running, the output directory will contain:

```
output/
├── patches/
│   ├── patches/       # Extracted image patches
│   ├── masks/         # Tissue masks
│   └── stitches/      # Optional tissue visualizations
├── features/          # Extracted feature files (.pt)
    ├── h5_files/      # Contains the HDF5 files with patch metadata (location, size, etc.)
    └── pt_files/      # Extracted feature tensors (.pt) used for model inference
├── process_slides.csv # CSV with processed slide IDs
└── inference_predictions.csv # Final slide-level predictions
```

### 📄 Description of Output Files

- **`inference_predictions.csv`**  
  Contains the final classification results for each slide. It includes the following columns:

  | Column                     | Description                                      |
  |----------------------------|--------------------------------------------------|
  | `slide_id`                 | ID of the processed slide                        |
  | `predicted_class`          | Predicted class label (0 or 1)                   |
  | `positive_class_probability` | Probability of the slide belonging to class 1 |
  
- **`process_slides.csv`**  
  Keeps track of which slides have already been processed, to avoid repeating patch extraction and feature computation.

  - ✅ If you add a new image, it will be appended to this file and processed as expected.
  - ⚠️ If you want to **re-run the full pipeline** (patch extraction + feature extraction + inference) on a slide that’s already in the file, you **must manually delete that row** from `process_slides.csv`.

---

## ⚙️ GPU vs CPU Execution

This container runs with **GPU if available** and **falls back to CPU automatically** if not.

### 🧠 Note on Performance

- **GPU recommended** for fast feature extraction with Vision Transformer
- **CPU works**, but feature extraction will be significantly slower

### 🛠️ Requirements for GPU Execution

To run with GPU, your local machine must have:

- A compatible **NVIDIA GPU**
- **NVIDIA drivers** installed
- **Docker Engine** installed
- **NVIDIA Container Toolkit** installed

#### Install NVIDIA Container Toolkit

```bash
sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker
```

#### Verify GPU Access from Docker

```bash
docker run --rm --gpus all nvidia/cuda:11.8.0-base nvidia-smi
```

If this command returns information about your GPU (model, memory, driver version, etc.), the system is correctly set up to use the GPU

> The inference script will automatically detect and use the GPU if available

---

## 🧪 Minimal Example

### 1. Create local folders:

```bash
mkdir input output
```

### 2. Copy your WSI slides (e.g. `.svs`) into `input/`.

```bash
cp your_slide.svs input/
```

### 3. Run the container:

```bash
docker run --rm \
  -v /path/to/local/input:/app/input \
  -v /path/to/local/output:/app/output \
  kalimuthu_infer_image \
  --patch_level 1
```

### 4. View results in `output/inference_predictions.csv`:

```bash
cat output/inference_predictions.csv
```

---

## 📬 Questions?

If you encounter any problems, please check:

- That your input slides are supported (`.svs`, `.tif`, etc.)
- That Docker has the right permissions to access input/output folders
- That your system has enough RAM/VRAM for patching and feature extraction

---

© 2025 – CLAM WSI Dockerized Inference

